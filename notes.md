# Azure Cloud Models and Service Types

`Public Cloud Model`
Widely Used
All resources are owned, operated, and managed by a third party service provider like Azure.
Cheapest

`Private Cloud Model`
Extra Security
More Compliance Adhering
Financial Institutions, Goverment Agencies

`Private Cloud Infrastructure`
Dedicated Data Center or Dedicated Network

`Hybrid Cloud Model`
Combinatation of Public and Private

# Full Featured Services

`Software As A Service (Saas)`
They are web based applications that require a subscription
Azure Batch
Azure AI

`Platform as a Service (PaaS)`
Operating systems that are hosted and managed for you, on top of which you can develop apps and services.

# Azure Support Services

Azure App Services
Azure Search

`Infrastructure as a Service (Iaas)`
Computing, storage and networking services that are hosted and managed for you, on top of which you can install operating systems and store data.

`Summary`
Consume Software as a Service
Build on Platform as a Service
Host on Infrastructure as a Service

# Azure Subscription

Allows creating and hosting cloud native applications with Azure App Services or Migrate SQL Database to an Azure SQL Database and let complex scaling and updating for you.
Azure High Performance Computing is available for those that do a lot of Graphics processing.

`Event-Driven Architecture`
Has a serverless compute platform

# Azure Resource Group

`What is an Azure Resource Group?`
Resource groups are logical groupings of related resources.
Related resources can mean those that belong to a single application, resources that are shared within the same team or a number of any other groupings. Say you have an eCommerce platform that sells dessert T-shirts. Now imagine wanting to manage the Azure app service it runs on, the SQL server and database storing its data, and the Azure blob storage account hosting its media files, all in a single place.
By putting them all in the same resource group, you can do just that. Or what if your networking team needed a consolidated and separated way to access your organization's application gateways, firewalls and virtual networks.
Because we know each team wants ownership of their own resources, let's say your DBA's also required exclusive access to your organization's SQL server instances and databases. Setting up this kind of separation is simple with resource groups. That's the beauty of resource groups. You decide what those logical groupings are and what works for your team in applications.
Besides organizing resources, one of the best things about resource groups is that you can handle them as a group. When resources are no longer needed, you can get rid of them all in one fell swoop. Need to update all of your dev environment resources? As long as they're within the same resource group, you can update them all in one go. One thing to be mindful of though, is that resource groups can't share a resource.
Resources can only be assigned to a single resource group at a time. And it makes sense. I can't put these objects in two boxes at the same time.
It's the same with resource groups. Restricting resources in this way preserves the integrity of the resource group and truly keeps it isolated from other groups. As we touched on earlier, resource groups are also a great way to manage your users access to resources. The role-based access control for example, you can grant access to specific resource groups or specific resources themselves. My team likes to separate resources by environment and application. And we control access to production with roles. With resource groups, you can similarly configure your team's access to fit your organization's policies.
Finally, Azure resource groups also takes advantage of tagging, which can be a powerful way to work with resources. By tagging resource groups with meaningful context like environment, workload, or owner, you can make it easier to identify how you use your resources, separate cost by departments that incur them, and take advantage of automation opportunities when used with continuous integration and continuous deployment processes. No matter what kind of applications you build, resources you use, or strategy you implement to manage resources within your team, resource groups give you the flexibility to maintain them that makes sense to you.

# Azure Resource Manager (ARM)
`Using the Azure Resource Manager`
Creating more and more resources, it can get time consuming to manage them. Especially if you had to do that one resource at a time.
Even with the help of resource groups, we'll realistically deal with several versions of our applications, different environments, and other configurations that increase the number of resources we use.
Azure provides the Azure Resource Manager as a convenient way to deal with this in a reliable and scalable way. the Azure Resource Manager is a collection of rest API endpoints that are used to create, update, and delete resources. Each request is authenticated and authorized by the same management layer. This results in a consistent experience for you no matter which tool or service you use to interact with the Azure Resource Manager. Most importantly, the Azure Resource Manager plays a critical role in scaling your deployment processes.
Due to its significance to resource management and deployment, it even has its own templates which are abbreviated and nicknamed ARM templates. These are JSON files that you can use to state your resource needs as code. By using these templates, you can repeat deployments and get the same results each time. Common tasks done at scale, like deploying 300 Windows VMs with tags or creating several manage clusters with Azure Container Service, are also easier because of these templates. This is due to the declarative syntax use.

`Declaritive Syntax`
With declarative syntax comes the ability to validate our templates and, ultimately, our deployments. If our templates are invalid, we never have to worry as only templates passing validation ever go through with deployment. ARM will even take care of deploying resources in parallel, if possible. It will also deploy interdependent resources in the correct order. ARM templates are great for consistently deploying one or more resources to a resource group or subscription.
ARM templates are the preferred way of managing resources in a production environment. Note, there are many developers preferring to use the Azure CLI instead. The main reason, ARM templates can quickly become overly verbose, large, and difficult to maintain especially as resources increase or become more interdependent. When ARM templates are this long and complicated, it takes much longer to interpret and edit, making it more likely for us to introduce it to errors. Rewritten as an Azure CLI script, however, many of these ARM templates can be translated into much shorter, legible, and maintainable files, which translates into a better experience for us.
Since the Azure Resource Manager is essentially a restful API, we can use it in a few different ways. Through the Azure portal, with the Azure CLI, or with PowerShell. The Azure portal, as you may have already experienced, is the central hub for managing you applications and resources. Many guided wizards, step by steps instructions, and informational descriptions are in the portal, which help you become more familiar with each resource.
Because of this, it's also a great way to become familiar with the Azure Resource Manager, especially if you're just getting started.
It's also simpler to use when managing and understanding complex infrastructures and deployment processes.

`Portal`
When using the Azure Resource Manager through the portal, you'll see that resources are organized into resource groups and by Azure services. These distinctions make infrastructure management at the application level, or by service type, possible and much more intuitive. One caveat to keep in mind, though, is that the portal is usually last when it comes to feature parody. That means that any functionality initially released through the Azure Resource Manager's original API may not be fully represented in the portal. It can also take up to 180 days after the initial release for it to be supported. If this is a concern, the following options may be more ideal. Another way to interact with the Azure Resource Manager is through the command line using PowerShell. This is a great option if you're already familiar with PowerShell, primarily work in a Windows environment, or want the option to imperatively write deployment scripts. On the other hand, if you work with multiple platforms and require cross-platform tooling that works exactly the same on all platforms, the Azure CLI is the best way to use the Azure Resource Manager. If you can't decide between the Azure CLI and PowerShell, the Azure CLI generally has shorter commands that are easier to remember. For me, that seals the deal with the Azure CLI. As a final factor, the Azure CLI is also the fastest tool to use for prototyping and exploration. This makes it my favorite way to use the Azure Resource Manager. No matter which way you choose to use the Azure Resource Manager, you'll reap the benefits of managing your resources in a consistent and scalable manner. Check out this code sample to see how easy it is to deploy a database with the Azure CLI. Then, you can explore the rest of the commands in the Azure CLI reference.


# What is an Azure Region?
Azure offers 55 regions worldwide, spread across 140 different countries. These regions are key to scaling your applications to your users around the world, fulfilling different compliance or data residency standards, and helping your applications stay online, even during outages or disasters. Regions also define how resources are divided and billed in Azure. So it's worth noting that some regions can have different operating costs. 
`Paired Regions`
With Azure Regions, you could also take advantage of a strategy called paired regions, which are two Azure Regions that are contained within a specific geographic boundary. East Asia and Southeast Asia, West India and South India, and West US to and West Central US, are some of the regional pairs available, with many more around the globe. The only exception to this geographic rule is Brazil South, as it's regional pair, South Central US is outside of it's geography. 
`Automatic Replication`
By using paired regions, your applications can take advantage of many features to help keep them online. One of them is automatic replication, which means your apps get replicated into both regions. So if you happen to have an outage in one region, your apps will still be online thanks to the other region. Or if you need to do some scheduled maintenance, it is strategically executed on one region within a pair at a time. Meaning no, or very minimal downtime for your applications, at all. 
When choosing which regions to host your applications, it's important to consider the following. If you are replacing your on-premises infrastructure, a closer region would be better to reduce network latency. Alternatively, if you are keeping your on-premises infrastructure, choosing a father region can provide the geographic redundancy benefits you need at a much lower cost. Your companies data requires also play a large role in choosing a region. And there may be legal reasons to choose a particular region as well. 
Stricter companies usually prefer data to be closer to home, forbidding storage outside of their geographic region. As a general rule, US companies require data within the US. European companies require their data within Europe and so on. Following similar rules, government access policies also influence the regions to choose. When it comes to your customers, the common practice is to choose regions that are closest to them, which is a good practice to follow. But, if you have multiple locations or locations that span across the country, what then? In this case, targeting your largest customer base and choosing the closest region to that base is ideal. Another option is to choose an Azure Region that is in between your customers' physical locations to provide the same performance for each location. When you create resources in Azure, be sure to keep these practices in mind when selecting the Azure Region they will be hosted on. Regions can be changed later on, but depending on the resource, it can be a major hassle to do so. And by hassle I mean, you have to open a ticket with Azure Support to make the change. When you're planning your applications and services, check out the Azure geographies pages for more details on each specific region.

# What is an Azure Availability Zone
Azure Availability Zones are unique physical locations within an Azure region. Each zone is comprised of one or more data centers that have their own power, cooling, and networking. All regions have at least three separate Availability Zones enabled, which makes them more reliable in case one zone fails. Since Availability Zones are physically separated, applications and data shouldn't be affected by other data center failures in the region. 
Every Availability Zone combines the capabilities of both a fault domain and an update domain, which are just fancy names for server racks set up to do specific things. 
The difference between them is that "fault domains" are used in unexpected outages, like when a rack's power is lost. While "update domains" are used to purposely take down an instance of your app in order to perform maintenance or apply patches. 
If you create five Azure functions across five zones in an Azure region, your functions would be distributed across five fault and update domains. By understanding how your resources are distributed, Azure can make sure that updates or deployments don't happen to different zones at the same time. Zones also offer zonal services and zone-redundant services. Zonal services allow you to host specific resources in a specific zone. So, if your legacy apps, the ones running on VMs, happen to be used the most within West US Two region, you could assign those VMs to a specific zone within that region or to have granular control with your "Azure Kubernetes Service Cluster", you could also specify which zones they are allowed to be distributed on. 
`Zone-Redundant Services` 
Zone-Reduntant Services are what enables automatic replication of your applications and data across zones. For your SQL database backups and redundancies or replicating your Azure Redis caches across zones, this service is invaluable. When you combine Availability Zones with region pairs, you can rest easy knowing that your high-availability strategy is reinforced. 
With your applications and data replicated on different zones within a region, and further replication across your region pairs, you can be sure that your applications are always available, thanks to the large network of redundancies you've just created. Almost all service and region combinations support Availability Zones. The only exception is Azure Cosmos DB in the Japan East, France Central, and North Europe regions. It's actually pretty easy to take advantage of all the benefits Availability Zones offer you. To see for yourself, check out this quick tutorial on how to create a Linux VM in an Availability Zone with the Azure CLI.


# What is an Azure App Service?
An Azure App Service is an HTTP based service that allows you to build and host web applications, mobile backends, and RESTful APIs. 
Azure App Service has support for .NET, .NET Core, Node JS, Python, Ruby, Java, and PHP, so there's a high likelihood that you can develop in your favorite language. 
You can also run PowerShell scripts or other executables as background services. This is great for handling legacy applications or parts of your app that can't be migrated to Azure App Service immediately, and once you're ready to deploy you can do so on either Windows or Linux environments. By building and hosting your applications with Azure App Service you can take advantage of so many services within Azure to make them better. 
`Azure DevOps`
One of the best services to use with your Azure App Service is Azure DevOps. From your commits, using whichever source control platform you prefer, all the way to deployment, Azure DevOps can fully automate your continuous integration and continuous delivery processes. 
Using Azure DevOps also allows you to set up multiple environments like Dev, QA, and Staging, which can be used to promote features and fixes through a validated process. If security and compliance are concerns for your application which we all know can be quite cumbersome then don't worry. Azure App Services are conformed to ICO, SOC and PCI standards by default. You can also setup IP restrictions, configure subdomains, or create blacklists and whitelists, among other compliance focused requirements. And as an extra convenience Azure App Services integrates natively with Azure Active Directory so you don't have to re-write any authentication mechanisms from scratch. 
`Hosting`
When hosting your apps with Azure App Services you can manage scale through the Azure CLI or the Azure portal. You can scale up or out, depending on your needs. Scale up to increase your CPU usage, memory, or disk space. This is great for enhancing your existing infrastructure. By scaling up I can now play my favorite game on ultra high video settings without the game stuttering or crashing and by scaling up your application it can handle higher loads and result in a better, faster experience for your users. Scaling out, on the other hand increases the number of instances we have to work with. So instead of replacing our existing graphics card with a better one, in this strategy we combine multiple lower end graphics cards to work together to help us play our game at an acceptable rate. 
This strategy is common in big data processing where multiple machines are linked together to collectively provide processing power. Depending on your pricing tier you can scale out anywhere from 20 instances to 100 instances. You can even do so automatically using pre-defined rules that you set up. If you're lucky enough to work on the Greenfield, Azure App Services also makes it simple to create new applications. Azure has a vast library of quick start templates. Using one of those templates you can literally create and deploy a web app, an Azure function, a REST API, or one of many other common applications with a single click. This is my favorite part about using Azure App Services as it allows me to get up and running quickly, especially when I'm prototyping or experimenting with new features. As developers Azure App Services helps us offload the management scaling and monitoring of the applications we build. Which means we can focus more on the code. That's all we really want, right? 

# Creating An App Service To Host An API Using Azure Cloud Shell. 
Using the Azure Cloud Shell we can quickly create an Azure app service to host an API. 
Open the Azure Cloud Shell by clicking on its icon located on the top menu. 
`AppService Plan`
To host an Azure app service we first need to create a plan. 
We can do this by using the AZ app service plan create command. 
Type az appservice plan create

Next, we will need to add some parameters about our plan. 
First, will be the name so type "--dev-apps-api" 
Next, we'll need to assign it to a resource group so use the resource group parameter. That's --resource-group. 
Also, since this is a dev API I will also tie it to my dev resource group. 

Lastly, we'll need to use the --sku parameter which is the pricing tier. 
In this case, I just want to use the FREE tier since this will be a dev instance. Done. 

`Creating The Web App To Host The API`
Next, we'll need to create the web app itself. The one that's going to host the api. 
Type az webapp create and then specify the resource group which will be dev for me, the plan which is the name of the plan I just created. 
So for me that's dev-cakes-api. The name of the web app itself which will be cakes for all. 
Finally, add the deployment local git parameter so that we can use git to deploy our app. 
That'll be --deployment-local-git. Then press Enter. 
Once this command finishes successfully you should receive some JSON output. 
All that's left is to find the deployment local git URL. 
Go ahead and copy that URL and you'll be able to add it as a git remote where you can push your API.

# What is Azure Blob Storage?
At the heart of many applications, Azure Blob Storage gives you a place to store large amounts of readily accessible data in the cloud. 
`One of five Azure storage types, blob storage deals primarily with large, unstructured data.`
Think videos, images, audio, text or even virtual hard drives for VMs. 
We use blob storage quite a bit. When do I choose blob storage versus file storage. Both sound like the same thing and they're both Azure storage types. Well blob storage is better suited for streaming video and audio. It's also great for serving images or large documents directly to a browser. 
If you need to provide availability to your storage account anywhere in the world, but with an HTTPS connection, well you guessed it. Blob storage is the better option. Because it's optimized for storing massive amounts of unstructured data, blob storage is also the better option for backups, disaster recovery and archiving of data. As the final deciding factor, blob storage is usually cheaper than Azure File Storage. 
`On the other hand, Azure File Storage is the preferred option in two primary scenarios.`
If your solutions are already integrated with the native file system APIs, and you're just looking to migrate to the cloud without having to re-architect your entire data storage model, files is the best option. 
Since it supports the standard server message blob protocol, Azure Files allow you to migrate to any other platform that uses the same standard. 
`SMB Protocol`
The SMB protocol also allows you to mount Azure Files Storage as a native share on a VM. Something you can't do with blob storage. 
Files storage is also the better option for network file shares. Many companies have internal file shares that store documents and files for easy distribution among employees. My company uses an internal file share to store company logos, job description templates and employee handbooks, among other things. 
With Azure File Storage, you can mount a file share in the cloud to the same drive loader that your on-premise application uses and it would work seamlessly. In general, applications or humans, that need to share files would benefit the most with Azure files storage. 
If you decided on blob storage, the next thing you'll need to think about is how often you'll need to access that data and help perform what that process should be. 
`Tiers`
With blob storage, there's this concept of access tiers and performance tiers. These tiers determine your costs, how easily you'll be able to access your data, and how performant your data store will be. Access tiers come in three forms, Hot, Cool and Archive. 
If I were to guess, GIPHY stores all of the office GIFs in the Hot access tier, which is perfect for storing data that is accessed frequently and actively. The storage cost are higher than the Cool and Archive tiers but it also has the lowest access cost. In other words, data marked at this tier is the easiest and quickest to access from blob storage. 
The cool access tier is better for data that is not accessed as frequently but is still expected to be available when it is. Think old high school photos on social media or short term backups of your applications data. Because of the longer access time frames, storage costs are lower than the Hot tier. However data in the Cool tier is expected to remain there for at least 30 days. 
Finally there's the Archive access tier. As the name implies, this data access tier is best suited for long term data storage. Think original data sets, full application backups and compliance data that needs to be kept but rarely accessed. This has the lowest storage cost of the three tiers. But the highest access cost. AKA this data takes the longest to retrieve. For the most cost effective way to store your data, it's a good idea to organize it into the appropriate tiers when implementing blob storage. Once you've categorized your data into the right access tiers, the next option to consider is its performance tier. 
`Performance Tiers`
Blob storage offers two performance tiers, standard and premium. Both offer high capacity and high throughput on large amounts of data. And in most cases, the standard tier will be just fine. There are however, a few scenarios where the premium performance tier makes more sense. Premium performance is optimized for high-transaction rates and consistent low-latency storage. Applications that require a lot of interactivity, user feedback and quick responsive updates are better suited to utilize the premium performance tier. Use cases like a mapping tool where massive amounts of data need to be instantly available based on user feedback, IoT analytics where thousands of write operations can occur a second or AI and ML processes, where massive amounts of different data types need to be consumed and processed in a rapid time frame. These would all be ideal candidates for the premium performance tier. Check out the Azure storage introduction page to learn more about the other three storage types, Queue, Table and Disk. Understanding the differences will help you choose the right storage type for your app.

# What is Cosmos DB?
Azure Cosmos DB is Microsoft's globally distributed database service. It was created to accommodate the high responsiveness and always on nature of most modern applications that live in the cloud. Cosmos DB is considered a NoSQL database and primarily works with four data models. 
`Four Data Models`
Document Store
  Data is schema-less and most likely stored as json documents 
Graph Oriented Model
  Data is represented as graph structures like nodes and edges
Key Value Store
  Which is the simplest form of a database management system and only stores pairs of keys and values
Wide Column Store
  Allows data with vast numbers of dynamic columns to be stored
Because of Cosmos DB's focus on high availability, geographic distribution, and speed, it comes with some pretty neat advantages. To start, Cosmos DB is classified as a foundational service within Azure, which means it's available in every Azure region. And with a literal click of a region on a map, you can add additional regions to replicate your data to. This makes scaling in new regions super simple. 
Cosmos DB also automatically indexes your data. This was especially helpful for smaller teams, like my own, where the responsibilities of database administration were usually left to us as developers. For most teams, the indexing Cosmos DB provides is enough, but if finer grain tuning and maintenance are needed, Cosmos DB gives you the option to do so. Another cool feature is that Cosmos DB can scale storage and throughput independently, something that's not as easy to do on other database models. 
This is because Cosmos DB uses something called request units to manage its level of performance. You can think of request units as Cosmos DB's performance currency, one that you have explicit control over. If your application suddenly had the need to store an additional terabyte of new product SKU's for example, you could easily add additional storage without having to pay for additional throughput. And alternatively, if your throughput needed to increase during weekends as your traffic spikes during these times, you could scale up your request units to maintain acceptable throughput levels without having to purchase additional storage. 
Lastly, Cosmos DB supports many popular open source API's and access methods. These include the Cassandra, Mongo DB, and Gremlin API's. Now, if you're like me, you may be excited to hear about all of these highlights and are beginning to wonder how to start using it. One thing must be made clear though. 
`Cosmos DB is not an alternative or replacement for SQL server.` 
There are very rare exceptions where you'd ever migrate your existing SQL server database to Cosmos DB. The only cases that come to mind would be if you were storing enormous chunks of json in a SQL server database, and improperly while you're at it. Or, if you were trying to achieve the CM globally distributed high transaction rate architecture with SQL server. Even then, migration to Cosmos DB is probably not the right answer unless a full rewrite of your data model and architecture comes first. With that out of the way, let's look into why you'd choose Cosmos DB over SQL server. 
The first thing to be aware of is that Cosmos DB is solely based in the cloud, while SQL server is obviously not. 
If you're creating cloud-first, globally available applications, Cosmos DB might be the better option for you. Another significant factor to keep in mind is that Cosmos DB supports a SQL like language to query your data. At first glance, this seemed to be a non issue and worked out well. I mean, who wants to learn yet another language? However, while your normal selects, joins, and basic SQL queries will work, many complex ones will not. For example, you can use the order by clause as you normally would, but many baffling restrictions apply. Sorting on multiple attributes are not possible, sorting on aliases are not possible, nor is it possible to sort by a computed value or anything other than a field name. These kinds of restrictions span many other normal scenarios that we've come to expect from the SQL language, making it a pain point of Cosmos DB. One of the most important distinctions however, between Cosmos DB and SQL server, is their scaling options. 
With SQL server, data consistency and integrity are the top priorities, so in order to scale with these assurances, SQL server handles transactions sequentially and usually commits them to a single server. This acts as the source of truth, which can then be replicated to other regions. To handle higher throughput, you would scale out and add more CPU or memory to hopefully make things faster. This only goes so far though as the performance scans you get from scaling out, only apply to read only activities. Inserts, updates, and deletes are not affected by these performance gains. By contrast, Cosmos DB scales out by adding more regions and subsequently, more machines. And then, mirrors its structure geographically. 
Because of this architecture, every node connects at both reads and writes, which makes it much faster. However, this also means that no single database can reject a change because of a violated policy, order cannot be ensured, and rollbacks to a certain point in time, say before some issues started occurring, are difficult to do. Despite these features, Cosmos DB is still a great choice for a highly available and always online database. Check out Cosmos DB global distribution overview page to learn more about the benefits of a globally distributed data store.

# Using Azure SQL
As SQL Database and SQL Server have evolved over the years, so have their deployment options. With Azure SQL Database, the SQL databases and SQL server instances you know can now be hosted and managed completely in the cloud. By going cloud first on your databases, you get the same benefits as any other cloud-first resource you use, like easy scaling, global availability, or hands-off administration. 
The two primary ways Azure SQL can be used to fit your data needs. Each determine how much control you have over the infrastructure resources and the cost. 
  The first option is through hosted infrastructure. Here SQL Server runs on Azure VMs. With this approach you have greater control over the data base engine, since you have access to the infrastructure it runs on. This option is great if you need the ability to fully customize your SQL server instance. It's also great for existing applications that require a quick migration to the cloud with barely any changes. And it's the best option if you need custom development or test environments and don't want to buy additional on-premise hardware, which can get real expensive. 
  The alternative options are to use Azure SQL databases or managed SQL instances. With these options, you give all the database maintenance tasks, like applying patches or upgrading SQL Server versions, to Azure instead of doing them yourself. You also get to use the latest stable SQL Server features, as the hardware and software you use will be fully managed, owned, and hosted by Microsoft.
`Deployment Options`  
For Azure SQL databases, there are three deployment options to choose from. 

First, there's the single database, which has its own set of resources managed by a database server. Single-database deployments are built for cloud-native applications and even have hyperscale and serverless tiers. Hyperscale is a special service tier in Azure SQL Database that basically removes any of the practical limits set on normal cloud databases. With hyperscale database deployments, a hundred terabyte databases, minute-long restores, and almost instantaneous backups are all possible. Serverless, within the context of Azure SQL Database, is a compute tier that's build for serverless applications. It scales compute resources based on workload demand and even pauses the database when it's not active, just like serverless apps do. 
Second, there's the elastic pool, which is a collection of databases with a shared set of resources. This is also managed by a database server. Elastic pool deployments provide a more cost-effective way to deal with multiple databases that have different usage patterns. This includes single-database types.  
Finally, there's the database server itself, which is used to manage the groups of single databases and elastic pools you may have. As you start thinking about potential database migrations to the cloud, or creating new ones, check out the Feature Comparison page for Azure SQL Database to see the full list of differences and to find the ones that matter to you.


What is an Azure Function?
Selecting transcript lines in this section will navigate to timestamp in the video
- While developing applications, there will be times where we just need to perform some sort of action. Maybe we need to copy a file for backup purposes, or we need to kickstart a web job based on some sort of state. Azure functions work well for these scenarios. With Azure functions, we only need to write the code needed to perform the action we want without requiring the full blown infrastructure that larger applications may need. At a minimum, Azure functions will always have two important parts. The code itself, and some sort of configuration file. Usually denoted as a function.json file. If you use a scripting language to write your code with, say, Powershell, you'll need to create this configuration file yourself. Otherwise, if using compiled languages like C Sharp or Java, this config file will be automatically generated for you from the annotations in your code. This config file holds several key pieces of information for your function. The most important of which is the bindings property. It's here that you'll set the binding type, the direction your function data flows, and the name of the bound data. It could also have more information depending on the binding type you choose. With just these two parts, your functions are ready to use. Say your application deals with many files uploads and each requires some scrubbing before proceeding. You could create a function that uses a blob trigger to do that, before adding it to an Azure blob storage container. How about using HTTP triggers to generate a coupon for a customer once they have finished their purchase? A function is a great use case for this smaller piece of functionality. There are many other types of triggers that Azure functions use, but we'll talk about them a bit later. Once you've created a function, you deploy them for use via a function app. Function apps can house multiple functions and is great for organizing related functions into a single, manageable, and logical unit. If you're thinking about using Azure functions, it's recommended to start with runtime version two, as most languages are supported and are in a stable state. Version three is available, but it's currently in preview, so it's best only if you want to experiment with Azure Functions that support .NET Core 3. As of this recording, version three is not recommended for use in production yet. When creating a function app, however, you won't have to worry, as it is set to version two automatically. It's also important to note that starting with version two of Azure Functions runtime, all functions in the same function app must be written in the same language. Before, the model allowed functions to be written in different languages. This is also good to keep in mind if you're migrating from a previous version to version two or higher. Azure Functions has two pricing models, and depending on how you use functions, one or the other may be better for you. There's the consumption plan, which only charges you based on the compute resources you use, meaning you only pay when your functions are running. With this plan, Azure automatically and dynamically scales instances of your function hosts as needed, even during periods of high load. This can be a great option if you have fluctuating workloads or can't otherwise comfortably predict expected usage. If your functions need a custom image to run or you already have existing VMs that are running other app services, then choosing the app service plan may be better for you. This allows you to manage your functions just like your other web apps. If you can't afford to wait for potential cold start delays, aka additional time spent waiting for Azure to wake up and kick start an instance, you can use the always on setting that is only available through the app service plan. To get started with Azure functions, you can download the Azure functions extension for VSCode, Azure functions core tools for use with the command prompt or terminal, or the Azure development workload for Visual Studio 2019. If you want to get coding right now, you can also head to the Azure portal and start coding your first function there.

Create a Function with a BlobTrigger
Selecting transcript lines in this section will navigate to timestamp in the video
- One of the most common uses for Azure functions is using it in conjunction with blob triggers. If you handle media or documents in your application, you'll most likely be utilizing Azure blob storage. Because of this, blob triggers present many use cases that extend your application's functionality. This could include sending notification emails to your sales team when certain clients upload files, copying the originally uploaded files to a backup directory, or kickstarting other processes in your platform. The platform I'm currently working on deals with many CSV and Excel files. Compared to the initial upload, these files end up as completely different files after all of the processing it goes through. Since our company requires a record of the initial state of all files we process, I'm going to create a function with a blob trigger to simply copy over any uploaded files into a backup directory. To start, we'll first need to create an Azure function app. This is the execution context our function will run in. Once logged in to the Azure portal, click on create a resource towards the top left of the menu. On the new page, find the function app resource, which is listed under the popular list. You can also search for function app in the search bar towards the top. Select function app. You'll be brought to the function app wizard, where you'll configure some settings. Choose your desired subscription and resource group with which you want to manage your function. You can also create a new resource group if you'd like. Give your function app a name, select code as the publish option, .NET Core as the runtime stack, and leave the default region of Central US selected. These settings correspond to how we'll build this function. Click next. Here, select the storage account that you'd like to use with your function app. Since I don't already have one, I'll go ahead and create a new one. As before, I'll leave the default options of Windows for my operating system, and the consumption type for my plan, as that's what we'll be using. Click next. Though optional, I'll leave application insights enabled for my function app, since this gives me invaluable insight if I have to debug something later on. Click review and create. Here, you'll have a chance to review the options you've just configured for your function app. You can easily go back and change something if you need to from this page. Otherwise, if everything looks good, click the create button to provision and deploy the function app. In a few minutes, your function app will be available. You can click on its status by selecting the notifications icon towards the top right corner of the portal. You'll know it's ready when you see the deployment succeeded message. Once your function app is ready, click on the go to resource button within the notification message. You can also find it by going to the all resources page and make it easier to get to by pinning it to your dashboard. Now that our function app is ready, all that's left is to create the actual function and utilize a blob trigger. In your function app, expand the contents located on the left. Next to functions, click the plus icon. Since this is our first function, select the in-portal option, then click continue. We are now asked to select a template that matches the kind of function we are creating. Select more templates, then click finish and view templates. Search for blob, and then select the Azure blob storage trigger template from the search results. You might be asked to install some extensions for Azure storage that you may not have. Go ahead and click install if this happens, and wait for the installation to succeed, then click continue. You'll now be asked to set up some information about the function. I'll use file-uploaded as my function name, add the path to my blob storage that I want monitored, and ensure the right storage account connection is selected. After reviewing that this information is correct, click create. Awesome, the function is ready. You may even see the boilerplate code that is automatically generated to log some information during the function's execution. Now we just have to write a bit more code to tell our function that we want a copy of any incoming file. Luckily, we can use bindings to achieve this. Bindings are an extremely useful feature in Azure functions, as they allow us to write much simpler code. First, we'll use the in binding to tell our function which source file to use. We denote this by using the blob trigger type followed by the path that we are expecting this file. Next, we'll also use the out binding to cleanly describe where we want our backup file to be copied to. We start with a blob type, followed by the path of the backup file's destination. We also explicitly declare write access using a trigger attribute. This will also be a stream type parameter. Finally, we await a copy function using the stream's CopyToAsync method. I'll even add some additional logs for a thorough history. That's it, make sure to click save towards the top before heading over to the Azure storage explorer to test out your function.

Other function trigger types
Selecting transcript lines in this section will navigate to timestamp in the video
- All Azure Functions require exactly one trigger, which is the action that starts the execution of your function code. There are a variety of trigger types available that work with many common scenarios. For example, TimerTriggers are great for executing functions on a predetermined schedule that you define. Use TimerTriggers to say, retrigger builds on Netlifly to publish scheduled blog posts, or to kick off a daily web job to process help request tickets. Another example are QueueTriggers, which allow you to respond to messages as they arrive in an Azure storage queue. These can be essential to functions that process a lot of data, like resolving matches for a catalog of products, or logging some metadata before moving the message to the next phase of a workflow. If a queue trigger happens to fail, Azure Functions automatically retries the function up to five more times before adding it to something called a poison queue. With this separation, you can even write another function to handle failed messages, giving you flexibility on how to handle these exceptions. CosmosDBTriggers are another trigger type available and are exactly what they sound like. They kickstart functions whenever additions or updates occur in a no SQL database. EvenGridTriggers respond to events delivered to a subscription in Azure Event Grid, an eventing service. You can subscribe to a multitude of events and kickstart your function based on those subscription's new notifications. By subscribing to a blog created event, for example, you can kickstart an image resizing function every time a new photo is uploaded to your Blob Storage. You can even customize your Event Grid subscription with subject filtering. That way, only events that match your filters execute your resizing function. EventHugTriggers respond to events delivered to Azure Event Hub, a big data streaming platform and event ingestion service. This is better suited for workflows working with IoT and application instrumentation. Azure Event Hub in particular has one distinct characteristic over Azure Event Grid, and that's ordering. Azure Event Hub processes events in order, which can be a good thing and a bad thing. If your process relies on a specific order like requiring specific approvals from specific people, then choosing Azure Event Hub to process those events along with EventHubTriggers to help you track each step may be the better solution for you. Just as there is support for normal storage queues via queue triggers, there's also support for service bus queues via ServiceBusQueueTriggers. Choosing between storage queues and service bus queues, which determines which triggers you should use, is highly dependent on what your application's needs are, so I won't go into too much detail about them here. You can check this link though to view a comparison of each solution and what considerations to keep in mind while choosing one. With so many trigger types, I'm sure you'll find the perfect ones to use with your Azure Functions.


When to use Durable Functions
Selecting transcript lines in this section will navigate to timestamp in the video
- When choosing between regular functions or durable functions. One of the key deciding factors is whether or not you're implementing a sequential, multi-step, workflow, as part of your application. If you have functionality in your app that depends on previous steps or specific states to trigger the next steps, durable functions make several tricky parts about these workflows much easier. Durable functions are an extension of Azure functions that allow you to write stateful functions in a server less compute environment. These are incredibly powerful when you have complex workflows or longer running dependent steps, and are the preferred solution over regular functions. At its core, durable functions provides a library of useful obstructions and tools that simplify workflow orchestration. This makes it easier to implement and define both workflow state and internal communication between functions. Which means they are also much easier to understand. Orchestration can occur with regular functions, but at the price of more overhead and a messier architecture. Durable functions give us a cleaner architecture by their very composition. All durable functions contain activity functions, and an orchestrator. Activity Functions are essentially the same thing as Azure functions. The difference here is that these activity functions can only be initiated by an activity trigger. The orchestrator, also known as an orchestrator function, manages the flow of execution and data of any associated activity functions. In doing so, it binds these activity functions into a single sequential workflow. Let's say we had an extremely basic workflow that handles orders at a bakery. This would start with charging a customer once they submit their online order, followed by an order request being generated for the pastry chefs to use and finally a text message to the customer once their order is ready. Even with only three steps, this workflow has a moderate amount of complexity within it. including a possible human interaction to trigger the final step. With workflows like these, durable functions allow us to easily implement an orchestrator pattern. But, without having to manage the messaging between functions ourselves, or worrying about workflow state the way we would if we implemented it in regular functions. Creating workflows using durable functions also gives you a consolidated view of a workflow because the orchestration of all the steps will be in a single place, the orchestrator function. And as an added bonus, this makes versioning your durable functions possible. Using durable functions with workflows also allows you to check the status of your workflow via REST API. 

A closer look at Durable Functions
Selecting transcript lines in this section will navigate to timestamp in the video
- Let's look at an example of a durable function in action. I travel a lot to speak at conferences and found that I go through a similar set of preparations for each one. I know that I have to book a flight, then book a hotel, and finally make a reservation at one the top-rated restaurants, yes it is a requirement. In essence, this easily translates to a workflow that I can implement with a durable function. Booking a flight, booking a hotel, and then making a reservation would all make perfect activity functions. An orchestrator function can manage this workflow and ensures my activity functions are performed in their proper order. If you're thinking, "But Adrienne, "this look like any other function "that uses a task-based API." It does but the implementation is actually quite different. Let's take a closer look. When the orchestrator makes the first invocation to book my flight, a queue message is actually sent from the orchestrator to the activity function. In turn, the activity function gets triggered by the queue message, hopefully completes booking the ticket, serializes the result and sends it back as another queue message to the orchestrator. When this message is received by the orchestrator, it triggers the second activity but with a clear difference. It actually restarts from the beginning, from the first line of the function. Since this booked ticket result is now assigned to the first variable, the orchestrator knows not to execute the booked ticket activity function again, and moves to the book hotel activity function instead. With this understanding of how orchestrators work in durable functions, it's important to keep a few things in mind. Because orchestrators will restart the execution flow each time it gets triggered, it is extremely important to write them in a deterministic way. This means ensuring that no matter how many times the orchestrator runs, its output should always be the same. So, using DateTime.Now, generating random GUIDs, or using multithread operations would not be advised as these do not result in a deterministic outputs. The durable task framework that durable functions uses will do its best to detect possible violations in your code that would affect this replayability in orchestrators. When it does find something, you'll receive a non-deterministic orchestration exception, but this obviously won't catch everything. So it's important to adhere to this deterministic constraint.


Provision a Virtual Machine
Selecting transcript lines in this section will navigate to timestamp in the video
- Virtual machines offer a great way to provide additional environments to work with. Especially if there are DEV or QA environments used during the development life cycle. I'll show you how easy it is to provision one in Azure. If you find yourself needing to provision one, you can do so graphically or via a CLA. As always, the Azure portal is one way to create resources. VMs included. Log into your Azure account. At the search bar towards the top, type "virtual machines" and select the search result under "services". On the virtual machines page, click add. As with the creation of all new resources, some basic information needs to be filled in. Make sure to select the correct subscription, and create a new resource group. Under "instance details", give your virtual machine a name, choose an appropriate region, and then choose the image type you need. Unless you need to change it, leave the other defaults as is. Next, you'll need to assign an administrator to this VM. Fill in the appropriate details under "administrator account" for a username and password. Be sure to make the password at least 12 characters long, and have it fulfill the complexity requirements. The last thing you'll need to configure are the inbound port rules. If you have specific requirements, you can set those here, otherwise you can use the common configuration. Choose the "allow selected ports" option, and then select "RGP (3389)" and "HTTP (80)" from the drop down. All that's left is to click the "review and create" button, and Azure will provision your VM. If you prefer the command line or want to get up and running fast, then the Azure Cloud Shell is another option for provisioning a VM. Start by going to http://www.shell.azure.com/bash If prompted, make sure to sign into your Azure account. You'll be asked to select "bash" or "power shell". To follow along with me, choose "bash". If you don't have a storage account connected, Azure will ask you to create a new one. Note that this does incur a small cost. Once you are connect to the Azure Cloud Shell, you'll need a total of two commands. Really, that's it. First, use the "az" group create command along with the name and location parameters. Once your resource group is created, use the "az VM" create command followed by the resource group name, image, admin username, and admin password parameters. 

Azure Resource Manager templates
Selecting transcript lines in this section will navigate to timestamp in the video
- Azure Resource Manager templates or ARM templates for short, are great for consistently deploying the same set of resources. Let's create one using the Azure portal. As you can see, I'm already logged into my Azure account. To build a custom ARM template from scratch. begin typing deploy a custom template into the main search bar located towards the top of the Azure portal. Select the search result under services. You'll be brought to the custom deployment wizard. As you can see, Azure provides a few options for you to choose from. You can build your own template in the editor. Use one or the common templates featured or even load one from GitHub. Keep in mind that if you do use a template from the GitHub community, it won't be supported by Microsoft. As with all open source resources, Be sure to event any potential templates you'd like to use for active owners and community, and a relatively recent change log. These can potentially mean that it's stable enough to use. For ARM templates, I found that it's always easier to use an available template and then modify it. Instead of building it from scratch. We want to create a web app. So select the Create a web app template. You'll now be brought to a wizard similar to the one to see when creating a new resource. Select the resource group we want to assign this web app to. As you can see, I have a few resource groups to choose from. I'll select the dev resource group. Then, towards the top, select the Edit template button. You'll now see the Edit template section and an editor that has your full template already populated. We'll get to that in a sec. On the top, you'll see option to add resources to your template, choose a quickstart template, which pulls from GitHub, Load Existing templates and a download option to save a copy of the template you're currently working on. along the left hand side of the editor, you'll see a quick summary of your templates, parameters, variables, and resources. They should already have some values since our template has been filled out. This acts as a helpful outline of your template. Okay, let's see what's in here. First, expand the parameter section. parameters are any values that can be used as inputs in our deployments. In our case, we only have one, which is the location parameter that holds 

